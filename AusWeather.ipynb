{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Final Project: Building a Rainfall Prediction Classifier\n",
    "Estimated time needed: **30** minutes\n",
    "    \n",
    "\n",
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "* Explore and perform feature engineering on a real-world data set\n",
    "* Build a classifier pipeline and optimize it using grid search cross validation\n",
    "* Evaluate your model by interpreting various preofmrance metrics and visualizations\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "TBW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"weatherAUS_2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop all rows with missing values\n",
    "To try to keep things simple we'll drop missing values and see what's left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we still have 56k observations left after dropping missing values, we may not need to impute any missing values.  \n",
    "Let's see how we do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1. Data leakage considerations\n",
    "Are there any barriers to being able to predict whether it will rain tomorrow given the available data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 Response\n",
    "If your goal is to predict attributes for tomorrow's weather, using data that relies on complete information from today's conditions is impractical. For instance, actual minimum and maximum temperatures for today cannot be determined until the day is complete, typically at midnight. Similarly, features such as RainToday, Rainfall, Evaporation, Sunshine, WindGustDir, and WindGustSpeed are not usable because they reflect the full day's conditions.\n",
    "\n",
    "Attributes recorded at specific times, like 9 am and 3 pm, are also restricted; using both would mean your prediction needs to be made after 3 pm.\n",
    "\n",
    "These factors highlight potential data leakage issues. Using data in training that would not be accessible in a real-world scenario for predicting tomorrow's rainfall can lead to misleading results.\n",
    "\n",
    "However, if we adjust our approach and aim to predict todayâ€™s rainfall using historical weather data up to and including yesterday, we can utilize all available features. This shift could be particularly useful for practical applications, such as deciding whether it's wise to bike to work.\n",
    "\n",
    "With this new target, we should update the names of the rain columns accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'RainToday': 'RainYesterday',\n",
    "                        'RainTomorrow': 'RainToday'\n",
    "                        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2. Data granularity\n",
    "Consider the Location field.   \n",
    "Do you think this all of the locations are useful information to include?  \n",
    "Any ideas on what to do to proceed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Response\n",
    "Would the weather patterns have the same predictability in vastly different locations in Australia? I would think not.  \n",
    "The chance of rain in one location can be much higher than in another. \n",
    "Using all of the locations requires a more complex model as it needs to adapt to local weather patterns.  \n",
    "Let's see how many observations we have for each location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location selection\n",
    "You could do some research to group cities in the `Location` column by distance, which I've done for you behind the scenes.  \n",
    "I found that Watsonia is only 15 km from Melbourne, and the Melbourne Airport is only 18 km from Melbourne.  \n",
    "Let's group these three locations together and use only their weather data to build our localized prediction model.  \n",
    "Because theere might still be some slight variations in the weather patterns we'll keep `Location` as a categorical variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.Location.isin(['Melbourne','MelbourneAirport','Watsonia',])]\n",
    "df. info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 7557 records, which should be enough to build a reasonably good model.  \n",
    "You could always gather more data if needed by updating it from the source.\n",
    "\n",
    "## Extracting a seasonality feature\n",
    "Now consider the `Date` column. We would expect the weather patterns to be seasonal, having different predictablitiy levels in winter and summer for example.  \n",
    "There may be some variation with `Year` as well, but we'll leave that for now.\n",
    "Let's engineer a `Season` feature from `Date` and drop `Date` afterward.  \n",
    "An easy way to do this is to define a function that assigns seasons to given months, then use that function to transform the `Date` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to map dates to seasons\n",
    "def date_to_season(date):\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    \n",
    "    if (month == 12) or (month == 1) or (month == 2):\n",
    "        return 'Summer'\n",
    "    elif (month == 3) or (month == 4) or (month == 5):\n",
    "        return 'Autumn'\n",
    "    elif (month == 6) or (month == 7) or (month == 8):\n",
    "        return 'Winter'\n",
    "    elif (month == 9) or (month == 10) or (month == 11):\n",
    "        return 'Spring'\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Apply the function to the 'Date' column\n",
    "df['Season'] = df['Date'].apply(date_to_season)\n",
    "\n",
    "df=df.drop(columns=['Date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a good set of features to work with. \n",
    "\n",
    "Let's go ahead and build our model.\n",
    "\n",
    "But wait, let's take a look at how well balanced our target is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RainToday'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. What can you conclude from these counts?\n",
    "- How often does it rain annualy in the Melbourne area?\n",
    "- How accurate would you be if you just assumed it won't rain every day?\n",
    "- Is this a balanced dataset?\n",
    "- Next steps?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Response\n",
    "Apparently it is usally not raining in the Melbourne area, although it rains on average about 30% of the days in a year.  \n",
    "Consider that if your model was super simplistic and always predicted that it will not rain today. You would be correct about 70% of the time. That's not a bad guess!  \n",
    "This is an important reminder that baseline accuracy isn't necessarily a good performance indicator for imbalanced data.  \n",
    "A model that always predicts the majority class might show high accuracy but lack real predictive power.\n",
    "\n",
    "Stratification is particularly critical when working with unbalanced data, as it helps the model learn from both classes adequately and ensures that the evaluation is fair, preventing overly optimistic results due to an imbalanced distribution in the splits. Since we have unbalanced classes in our target, we need to stratify the target so it has the same proportion of classes in in both the training and the testing sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4. Define the feature and target dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Exercise 4. Define the feature and target dataframes\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Complete the followng code:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# X = df.drop(columns='', axis=1)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# y = df['']\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m### Exercise 4 Response\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRainToday\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRainToday\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m### Exercise 5. Split data into training and test sets, ensuring target stratification\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Complete the followng code:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#  X_train, X_test, y_train, y_test = train_test_split(..., ..., test_size=0.2, stratify=..., random_state=42)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m### Exercise 5 Response\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Complete the followng code:\n",
    "# X = df.drop(columns='', axis=1)\n",
    "# y = df['']\n",
    "\n",
    "### Exercise 4 Response\n",
    "X = df.drop(columns='RainToday', axis=1)\n",
    "y = df['RainToday']\n",
    "\n",
    "\n",
    "### Exercise 5. Split data into training and test sets, ensuring target stratification\n",
    "# Complete the followng code:\n",
    "#  X_train, X_test, y_train, y_test = train_test_split(..., ..., test_size=0.2, stratify=..., random_state=42)\n",
    "\n",
    "### Exercise 5 Response\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "\n",
    "# Automatically detect numerical and categorical columns and assign them to separate numeric and categorical features\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "\n",
    "## Define preprocessing pipelines for both feature types\n",
    "\n",
    "### Scale the numeric features\n",
    "# Preprocessing pipeline for numeric features\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "### One-hot encode the categoricals \n",
    "# Preprocessing pipeline for categorical features\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "\n",
    "### Combine the transformers into a single column transformer\n",
    "# We'll use the sklearn \"column transformer\" estimator to separately transform the features, which will then concatenate the output as a single feature space.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "### Now let's create a pipeline by combining the preprocessing with a Random Forest classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define a parameter grid to use in a cross validation grid search model optimizer\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "### Pipeline usage in crossvalidation\n",
    "# Recall that the pipeline is repeatedly used within the crossvalidation to fit on each internal training fold and predict on its corresponding validation fold\n",
    "\n",
    "## Perform grid search cross-validation and fit the best model to the training data\n",
    "### Select a cross-validation method, ensuring target stratification during validation\n",
    "\n",
    "scv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "## Exercise 6. Instantiate and fit GridSearchCV to the pipeline\n",
    "\n",
    "# Complete the followng code:\n",
    "# grid_search = GridSearchCV(..., param_grid, cv=..., scoring='accuracy', verbose=2)\n",
    "# grid_search.fit(..., ...)\n",
    "\n",
    "### Exercise 6 Response\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=scv, scoring='accuracy', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "### Print the best parameters and best crossvalidation score\n",
    "print(\"\\nBest parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "## Exercise 7. Display your model's estimated score\n",
    "# Complete the followng code:\n",
    "# test_score = grid_search.score(X_test, y_test)\n",
    "# print(\"Test set score: {:.2f}\".format(test_score))\n",
    "\n",
    "### Exercise 7 Response\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test set score: {:.2f}\".format(test_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a reasonably accurate classifer, which is expected to correctly predict about 84% of the time whether it will rain today.  \n",
    "Let's take a deeper look at the results.\n",
    "\n",
    "The best model is stored within the gridsearch object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 8. Get the model predictions from the grid search estimator on the unseen data\n",
    "# Complete the followng code:\n",
    "# y_pred = grid_search.predict(...)\n",
    "\n",
    "### Exercise 8 Response\n",
    "y_pred = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9. Print the classification report\n",
    "# Complete the followng code:\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(...(y_test, y_pred))\n",
    "\n",
    "### Exercise 9 Response\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 10. Plot the confusion matrix \n",
    "# Complete the followng code:\n",
    "# conf_matrix = ...(y_test, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=...)\n",
    "# disp.plot(cmap='Blues')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "### Exercise 10 Response\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances\n",
    "Recall that to obtain the categorical feature importances, we have to work our way backward through the modelling pipeline to associate the feature importances with their original input variables, not the one-hot encoded ones. We don't need to do this for the numeric variables because we didn't modify their names in any way.  \n",
    "Remmeber we went from categorical features to one-hot encoded features, using the 'cat' column transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_['preprocessor'].named_transformers_['cat'].named_steps['onehot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cool! Now let's get all of the feature importances and associate them with the original features\n",
    "\n",
    "### Exercise 11. Extract the feature importances\n",
    "# Complete the followng code:\n",
    "# feature_importances = grid_search.best_estimator_['classifier']. ...\n",
    "\n",
    "### Exercise 11 Response\n",
    "feature_importances = grid_search.best_estimator_['classifier'].feature_importances_\n",
    "\n",
    "\n",
    "# Combine numeric and categorical feature names\n",
    "feature_names = numeric_features + list(grid_search.best_estimator_['preprocessor']\n",
    "                                        .named_transformers_['cat']\n",
    "                                        .named_steps['onehot']\n",
    "                                        .get_feature_names_out(categorical_features))\n",
    "\n",
    "# Define a ranked feature importance DataFrame \n",
    "importance_df = pd.DataFrame({'Feature': feature_names,\n",
    "                              'Importance': feature_importances\n",
    "                             }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "N = 20  # Change this number to display more or fewer features\n",
    "top_features = importance_df.head(N)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show the most important feature on top\n",
    "plt.title(f'Top {N} Most Important Features in predicting whether it will rain today')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()\n",
    "# Print test score summary\n",
    "print(f\"\\nTest set accuracy: {test_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can get the feature names from the one-hot encoder\n",
    "grid_search.best_estimator_['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "edc9885faaca0dc785cf1ac7b2bbdf419c6b5f5692275d5a30b1949ec8ada66d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
